{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8322a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9d269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=C:\\Users\\Kami\\anaconda3\\envs\\robot_grasping\\lib\\site-packages\\pybullet_envs\\bullet\n"
     ]
    }
   ],
   "source": [
    "from pybullet_envs.bullet.kukaGymEnv import KukaGymEnv\n",
    "import random\n",
    "import os\n",
    "from gym import spaces\n",
    "import time\n",
    "import pybullet as p\n",
    "from pybullet_envs.bullet import kuka\n",
    "import numpy as np\n",
    "import pybullet_data\n",
    "import pdb\n",
    "import distutils.dir_util\n",
    "import glob\n",
    "from pkg_resources import parse_version\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc3477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KukaDiverseObjectEnv(KukaGymEnv):\n",
    "    def __init__(self,\n",
    "                urdfRoot=pybullet_data.getDataPath(),\n",
    "                actionRepeat=80,\n",
    "                isEnableSelfCollision=True,\n",
    "                renders=False,\n",
    "                isDiscrete=False,\n",
    "                maxSteps=8,\n",
    "                dv=0.06,\n",
    "                removeHeightHack=False,\n",
    "                blockRandom=0.3,\n",
    "                cameraRandom=0,\n",
    "                width=48,\n",
    "                height=48,\n",
    "                numObjects=5,\n",
    "                isTest=False): #,\n",
    "#                 from_pixels=False):\n",
    "        \n",
    "        self._isDiscrete = isDiscrete\n",
    "        self._timeStep = 1. / 240.\n",
    "        self._urdfRoot = urdfRoot\n",
    "        self._actionRepeat = actionRepeat\n",
    "        self._isEnableSelfCollision = isEnableSelfCollision\n",
    "        self._observation = []\n",
    "        self._envStepCounter = 0\n",
    "        self._renders = renders\n",
    "        self._maxSteps = maxSteps\n",
    "        self.terminated = 0\n",
    "        self._cam_dist = 1.3\n",
    "        self._cam_yaw = 180\n",
    "        self._cam_pitch = -40\n",
    "        self._dv = dv\n",
    "        self._p = p\n",
    "        self._removeHeightHack = removeHeightHack\n",
    "        self._blockRandom = blockRandom\n",
    "        self._cameraRandom = cameraRandom\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._numObjects = numObjects\n",
    "        self._isTest = isTest\n",
    "        \n",
    "#         self._from_pixels = from_pixels\n",
    "        \n",
    "        \n",
    "        if self._renders:\n",
    "            self.cid = p.connect(p.SHARED_MEMORY)\n",
    "            if (self.cid < 0):\n",
    "                self.cid = p.connect(p.GUI)\n",
    "            p.resetDebugVisualizerCamera(1.3, 180, -41, [0.52, -0.2, -0.33])\n",
    "        \n",
    "        else:\n",
    "            self.cid = p.connect(p.DIRECT)\n",
    "        self.seed()\n",
    "        \n",
    "        if (self._isDiscrete):\n",
    "            if self._removeHeightHack:\n",
    "                self.action_space = spaces.Discrete(9)\n",
    "            else:\n",
    "                self.action_space = spaces.Discrete(7)\n",
    "        else:\n",
    "            self.action_space = spaces.Box(low=-1, high=1, shape=(3,)) # dx, dy, da\n",
    "            if self._removeHeightHack:\n",
    "                self.action_space = spaces.Box(low=-1, high=1, shape=(4,))  # dx, dy, dz, da\n",
    "                \n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self._height,\n",
    "                                                                self._width,\n",
    "                                                                3))\n",
    "        self.viewer = None\n",
    "    \n",
    "    def reset(self):\n",
    "        look = [0.23, 0.2, 0.54]\n",
    "        distance = 1.\n",
    "        pitch = -56 + self._cameraRandom * np.random.uniform(-3, 3)\n",
    "        yaw = 245 + self._cameraRandom * np.random.uniform(-3, 3)\n",
    "        roll = 0\n",
    "        self._view_matrix = p.computeViewMatrixFromYawPitchRoll(look, distance, yaw, pitch, roll, 2)\n",
    "        fov = 20. + self._cameraRandom * np.random.uniform(-2, 2)\n",
    "        aspect = self._width / self._height\n",
    "        near = 0.01\n",
    "        far = 10\n",
    "        self._proj_matrix = p.computeProjectionMatrixFOV(fov, aspect, near, far)\n",
    "\n",
    "        self._attempted_grasp = False\n",
    "        self._env_step = 0\n",
    "        self.terminated = 0\n",
    "\n",
    "        p.resetSimulation()\n",
    "        p.setPhysicsEngineParameter(numSolverIterations=150)\n",
    "        p.setTimeStep(self._timeStep)\n",
    "        p.loadURDF(os.path.join(self._urdfRoot, \"plane.urdf\"), [0, 0, -1])\n",
    "\n",
    "        p.loadURDF(os.path.join(self._urdfRoot, \"table/table.urdf\"), 0.5000000, 0.00000, -.820000,\n",
    "                   0.000000, 0.000000, 0.0, 1.0)\n",
    "\n",
    "        p.setGravity(0, 0, -10)\n",
    "        self._kuka = kuka.Kuka(urdfRootPath=self._urdfRoot, timeStep=self._timeStep)\n",
    "        self._envStepCounter = 0\n",
    "        p.stepSimulation()\n",
    "        \n",
    "        urdfList = self._get_random_object(self._numObjects, self._isTest)\n",
    "        self._objectUids = self._randomly_place_objects(urdfList)\n",
    "        self._observation = self._get_observation()\n",
    "        return np.array(self._observation)\n",
    "\n",
    "    def _randomly_place_objects(self, urdfList):\n",
    "        objectUids = []\n",
    "        for urdf_name in urdfList:\n",
    "            xpos = 0.4 + self._blockRandom * random.random()\n",
    "            ypos = self._blockRandom * (random.random() - .5)\n",
    "            angle = np.pi / 2 + self._blockRandom * np.pi * random.random()\n",
    "            orn = p.getQuaternionFromEuler([0, 0, angle])\n",
    "            urdf_path = os.path.join(self._urdfRoot, urdf_name)\n",
    "            uid = p.loadURDF(urdf_path, [xpos, ypos, .15], [orn[0], orn[1], orn[2], orn[3]])\n",
    "            objectUids.append(uid)\n",
    "            \n",
    "            for _ in range(500):\n",
    "                p.stepSimulation()\n",
    "        return objectUids\n",
    "    \n",
    "    def _get_observation(self):\n",
    "#         if self._from_pixels:\n",
    "#             obs = super()._get_observation()\n",
    "        img_arr = p.getCameraImage(width=self._width,\n",
    "                                  height=self._height,\n",
    "                                  viewMatrix=self._view_matrix,\n",
    "                                  projectionMatrix=self._proj_matrix)\n",
    "        rgb = img_arr[2]\n",
    "        np_img_arr = np.reshape(rgb, (self._height, self._width,4))\n",
    "        pixel = np_img_arr[:, :, :3]\n",
    "        pixel = pixel.transpose(2,0,1)\n",
    "        state = self._kuka.endEffectorPos + [self._kuka.endEffectorAngle]\n",
    "        for uid in self._objectUids:\n",
    "            pos, quaternion = self._p.getBasePositionAndOrientation(uid)\n",
    "            state += list(pos)\n",
    "            state += list(quaternion)\n",
    "        state = np.array(state)\n",
    "        return pixel, state\n",
    "    \n",
    "    def step(self, action):\n",
    "        dv = self._dv\n",
    "        if self._isDiscrete:\n",
    "            assert isinstance(action, int)\n",
    "            if self._removeHeightHack:\n",
    "                dx = [0, -dv, dv, 0, 0, 0, 0, 0, 0][action]\n",
    "                dy = [0, 0, 0, -dv, dv, 0, 0, 0, 0][action]\n",
    "                dz = [0, 0, 0, 0, 0, -dv, dv, 0, 0][action]\n",
    "                da = [0, 0, 0, 0, 0, 0, 0, -0.25, 0.25][action]\n",
    "            else:\n",
    "                dx = [0, -dv, dv, 0, 0, 0, 0][action]\n",
    "                dy = [0, 0, 0, -dv, dv, 0, 0][action]\n",
    "                dz = -dv\n",
    "                da = [0, 0, 0, 0, 0, -0.25, 0.25][action]\n",
    "        else:\n",
    "            dx = dv * action[0]\n",
    "            dy = dv * action[1]\n",
    "            if self._removeHeightHack:\n",
    "                dz = dv * action[2]\n",
    "                da = 0.25 * action[3]\n",
    "            else:\n",
    "                dz = -dv\n",
    "                da = 0.25 * action[2]\n",
    "        \n",
    "        return self._step_continuous([dx, dy, dz, da, 0.3])\n",
    "    \n",
    "    def _step_continuous(self, action):\n",
    "        self._env_step += 1\n",
    "        self._kuka.applyAction(action)\n",
    "        for _ in range(self._actionRepeat):\n",
    "            p.stepSimulation()\n",
    "            if self._renders:\n",
    "                time.sleep(self._timeStep)\n",
    "            if self._termination():\n",
    "                break\n",
    "                \n",
    "        state = p.getLinkState(self._kuka.kukaUid, self._kuka.kukaEndEffectorIndex)\n",
    "        end_effector_pos = state[0]\n",
    "        if end_effector_pos[2] <= 0.1:\n",
    "            finger_angle = 0.3\n",
    "            for _ in range(500):\n",
    "                grasp_action = [0, 0, 0, 0, finger_angle]\n",
    "                self._kuka.applyAction(grasp_action)\n",
    "                p.stepSimulation()\n",
    "                finger_angle -= 0.3 / 100.\n",
    "                if finger_angle < 0:\n",
    "                    finger_angle = 0\n",
    "            for _ in range(500):\n",
    "                grasp_action = [0, 0, 0.001, 0, finger_angle]\n",
    "                self._kuka.applyAction(grasp_action)\n",
    "                p.stepSimulation()\n",
    "                if self._renders:\n",
    "                    time.sleep(self._timeStep)\n",
    "                finger_angle -= 0.3 / 100.\n",
    "                if finger_angle < 0:\n",
    "                    finger_angle = 0\n",
    "            self._attempted_grasp = True\n",
    "        observation = self._get_observation()\n",
    "        done = self._termination()\n",
    "        reward = self._reward()\n",
    "\n",
    "        debug = {'grasp_success': self._graspSuccess}\n",
    "        return observation, reward, done, debug\n",
    "    \n",
    "    def _reward(self):\n",
    "        reward = 0\n",
    "        self._graspSuccess = 0\n",
    "        for uid in self._objectUids:\n",
    "            pos, _ = p.getBasePositionAndOrientation(uid)\n",
    "            if pos[2] > 0.2:\n",
    "                self._graspSuccess += 1\n",
    "                reward = 1\n",
    "                break\n",
    "        return reward\n",
    "    \n",
    "    def _termination(self):\n",
    "        return self._attempted_grasp or self._env_step >= self._maxSteps\n",
    "    \n",
    "    def _get_random_object(self, num_objects, test):\n",
    "        if test:\n",
    "            urdf_pattern = os.path.join(self._urdfRoot, 'random_urdfs/*0/*.urdf')\n",
    "        else:\n",
    "            urdf_pattern = os.path.join(self._urdfRoot, 'random_urdfs/*[1-9]/*.urdf')\n",
    "        found_object_directories = glob.glob(urdf_pattern)\n",
    "        total_num_objects = len(found_object_directories)\n",
    "        selected_objects = np.random.choice(np.arange(total_num_objects), num_objects)\n",
    "        selected_objects_filenames = []\n",
    "        for object_index in selected_objects:\n",
    "            selected_objects_filenames += [found_object_directories[object_index]]\n",
    "        return selected_objects_filenames\n",
    "    \n",
    "    if parse_version(gym.__version__) < parse_version('0.9.6'):\n",
    "        _reset = reset\n",
    "        _step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d44ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KukaDiverseObjectEnv(renders=False,\n",
    "                           width=64,\n",
    "                           height=64,\n",
    "                           numObjects=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb28736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-04584f7899e8>:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(self._observation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([array([[[185, 185, 186, ..., 225, 230, 234],\n",
       "               [181, 183, 192, ..., 226, 231, 234],\n",
       "               [186, 183, 188, ..., 227, 231, 234],\n",
       "               ...,\n",
       "               [171, 173, 172, ...,  93,  96,  99],\n",
       "               [172, 175, 176, ...,  94,  97, 100],\n",
       "               [176, 175, 174, ...,  95,  99, 101]],\n",
       "\n",
       "              [[156, 155, 158, ...,  95,  97,  99],\n",
       "               [152, 155, 163, ...,  95,  97,  99],\n",
       "               [158, 156, 161, ...,  96,  97,  99],\n",
       "               ...,\n",
       "               [147, 150, 149, ..., 130, 135, 139],\n",
       "               [149, 152, 153, ..., 132, 137, 141],\n",
       "               [151, 151, 151, ..., 134, 138, 142]],\n",
       "\n",
       "              [[124, 125, 126, ...,   8,   9,   9],\n",
       "               [123, 127, 133, ...,   8,   9,   9],\n",
       "               [130, 126, 131, ...,   8,   9,   9],\n",
       "               ...,\n",
       "               [118, 121, 121, ..., 187, 194, 200],\n",
       "               [120, 123, 124, ..., 190, 196, 202],\n",
       "               [123, 124, 123, ..., 192, 198, 203]]], dtype=uint8),\n",
       "       array([ 0.537     ,  0.        ,  0.5       ,  0.        ,  0.64888983,\n",
       "              -0.13535551, -0.14221946,  0.02778541, -0.39063273,  0.75476303,\n",
       "               0.52627637])                                                   ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3636a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU IS AVAILABLE :D\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU IS AVAILABLE :D') \n",
    "else:  \n",
    "    device = torch.device(\"cpu\") \n",
    "    print('GPU not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdb4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('pixel',\n",
    "                        'state',\n",
    "                        'action',\n",
    "                        'next_pixel',\n",
    "                        'next_state',\n",
    "                        'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"save a transition\"\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b39cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQN(nn.Module):\n",
    "    def __init__(self): #, h, w):\n",
    "        super(GQN, self).__init__()\n",
    "        \n",
    "        # vision net\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=6, stride=2) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.drop1 = nn.Dropout2d(0.3)\n",
    "        self.fc1 = nn.Linear(32*12*12, 128)\n",
    "        \n",
    "\n",
    "        # motor net\n",
    "        self.fc2 = nn.Linear(11+3, 64) # combining state obs and action\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        \n",
    "        # grasp net\n",
    "        self.fc4 = nn.Linear(128 + 32, 32)\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, pixel, state, a): # s is the observation, a is the action \n",
    "        pixel = pixel.to(device)\n",
    "        state = state.to(device)\n",
    "        a = a.to(device)\n",
    "        combined = torch.cat((state, a), axis=1)\n",
    "        \n",
    "        # vision net\n",
    "        pixel = F.relu(self.bn1(self.conv1(pixel))) # 30, 30\n",
    "        pixel = F.relu(self.bn2(self.conv2(pixel))) # 26, 26\n",
    "        pixel = F.relu(self.bn3(self.conv3(pixel))) # 24, 24\n",
    "        pixel = F.max_pool2d(pixel, kernel_size=2) # 12, 12\n",
    "        pixel = self.drop1(pixel)\n",
    "        pixel = pixel.reshape((-1, 32*12*12))\n",
    "        pixel = F.relu(self.fc1(pixel))\n",
    "        \n",
    "        # motor net\n",
    "        combined = F.relu(self.fc2(combined))\n",
    "        combined = self.drop2(combined)\n",
    "        combined = F.relu(self.fc3(combined))\n",
    "        \n",
    "        # grasp net\n",
    "        val = torch.cat((pixel, combined), axis=1)\n",
    "        val = F.relu(self.fc4(val))\n",
    "        \n",
    "        return self.fc5(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79fbd393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-94382ee74414>:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(self._observation)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 50\n",
    "\n",
    "policy_net = GQN().to(device) \n",
    "target_net = GQN().to(device) \n",
    "target_net.load_state_dict(policy_net.state_dict()) \n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayBuffer(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119bec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c505287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a119ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyMethod(state,\n",
    "                       sample_fn,\n",
    "                      objective_fn,\n",
    "                      update_fn,\n",
    "                      initial_params,\n",
    "                      num_elites,\n",
    "                      num_iterations=1,\n",
    "                      threshold_to_terminate=None):\n",
    "    \n",
    "    updated_params = initial_params\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        samples = sample_fn(**updated_params) # used for dictionaries to give arguments\n",
    "        \n",
    "        values = objective_fn(samples, state)\n",
    "        \n",
    "        if isinstance(samples, dict):\n",
    "            sample_order = [i for i, _ in sorted(enumerate(values), key=operator.itemgetter(1))]\n",
    "           \n",
    "            sorted_samples = {k: [v[i] for i in sample_order] for k,v in sample.items()}\n",
    "            \n",
    "            elite_samples = {k:v[-num_elites:] for k, v in sorted_samples.items()}\n",
    "            \n",
    "        else:\n",
    "            sorted_samples = [s for s, _ in sorted(zip(samples,values), key=operator.itemgetter(1))]\n",
    "                \n",
    "            elite_samples = sorted_samples[-num_elites:]\n",
    "                \n",
    "        updated_params = update_fn(elite_samples)\n",
    "            \n",
    "        if ((threshold_to_terminate is not None) and (max(values) > threshold_to_terminate)):\n",
    "            break\n",
    "                \n",
    "    return samples, values, updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM_policy(object):\n",
    "    def __init__(self,\n",
    "                state, \n",
    "                batch_size=64,\n",
    "                action_size=3):\n",
    "    \n",
    "        self.batch_size = batch_size\n",
    "        self._action_size = action_size\n",
    "\n",
    "        self._action_space = spaces.Box(low=-1, high=1, shape=(3,))\n",
    "        \n",
    "    def sample_action(self, state):\n",
    "            \n",
    "        def objective_fn(sample, pixel, state):\n",
    "            pixel = pixel.title(64,).reshape(64, 3, 64, 64)\n",
    "            state = state.tile(64,).reshape(64, 11)\n",
    "            sample = torch.tensor(sample, dtype=torch.float)\n",
    "            q_values = policy_net(state, sample)\n",
    "            \n",
    "            return q_values\n",
    "\n",
    "        def sample_fn(mean, std):\n",
    "            return mean + std * np.random.randn(self.batch_size, self._action_size)\n",
    "\n",
    "        def update_fn(elite_samples):\n",
    "            return {'mean': np.mean(elite_samples, axis=0),\n",
    "                   'std': np.std(elite_samples, axis=0, ddof=1)}\n",
    "\n",
    "        global steps_done\n",
    "        mu = np.zeros(3)\n",
    "        mu[2] = -1\n",
    "        initial_params = {'mean': mu, 'std': .5 * np.ones(3)}\n",
    "        sample, values, final_params = CrossEntropyMethod(state,\n",
    "                                                          sample_fn,\n",
    "                                                         objective_fn,\n",
    "                                                         update_fn,\n",
    "                                                         initial_params,\n",
    "                                                         num_elites=10,\n",
    "                                                         num_iterations=3)\n",
    "        \n",
    "        idx = torch.argmax(values.detach().cpu())\n",
    "        best_cont_actions, best_cont_vals = sample[idx], values[idx]\n",
    "        steps_done += 1\n",
    "\n",
    "        return best_cont_actions\n",
    "\n",
    "CEM = CEM_policy(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE) \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    non_final_next_pixels = torch.cat([s for s in batch.next_pixel if s is not None])\n",
    "    \n",
    "    pixel_batch = torch.cat(batch.pixel)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(pixel_batch, state_batch, action_batch) #.gather(1, action_batch) \n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states, action_batch).max(1)[0].detach() \n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch \n",
    "    \n",
    "    criterion = nn.SmoothL1Loss() \n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b296351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_episodes=50, max_episode_length=15):\n",
    "    successes = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        \n",
    "        for t in range(max_episode_length):\n",
    "            action = CEM.sample_action(state)\n",
    "            (next_pixel, next_state), reward, done, _ = env.step(action)\n",
    "            \n",
    "            if reward == 1:\n",
    "                successes += 1\n",
    "            \n",
    "            pixel = torch.tensor(next_pixel, dtype=torch.float)\n",
    "            state = torch.tensor(next_state, dtype=torch.float)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    return successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cecbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agent(num_episodes=1, max_episode_length=15, save_every=500):\n",
    "    episode_durations = []\n",
    "    eps_history = []\n",
    "    rewards = []\n",
    "    success_rates = []\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        pixel, state = env.reset()\n",
    "        pixel = torch.tensor(pixel, dtype=torch.float)\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        \n",
    "        for t in range(max_episode_length):\n",
    "            action = CEM.sample_action(pixel, state)\n",
    "            (next_pixel, next_state), reward, done, _ = env.step(action) \n",
    "            \n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            next_pixel = torch.tensor(next_pixel, dtype=torch.float)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "            action = torch.tensor(action, dtype=torch.float)\n",
    "            memory.push(pixel.unsqueeze(0), state.unsqueeze(0), action.unsqueeze(0), next_state.unsqueeze(0), reward)\n",
    "            \n",
    "            pixel = next_pixel\n",
    "            state = next_state\n",
    "            \n",
    "            optimize_model()\n",
    "            \n",
    "            if done:\n",
    "                episode_durations.append(t+1)\n",
    "                break\n",
    "        \n",
    "        rewards.append(reward.item())\n",
    "        \n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        if i_episode % save_every == 0:\n",
    "            torch.save(policy_net.state_dict(), 'State pixel Policy-net DQN.pt')\n",
    "            torch.save(target_net.state_dict(), 'State pixel Target-net DQN.pt')\n",
    "        \n",
    "        if (i_episode + 1) % 125 == 0:\n",
    "            success_per_1000 = test()\n",
    "            success_rates.append(success_per_1000)\n",
    "            print(\"Episode: \", i_episode + 1, \"/\", num_episodes, (success_per_1000/50)*100)\n",
    "    \n",
    "    final_success = test()\n",
    "    success_rates.append(final_success)\n",
    "    \n",
    "    torch.save(policy_net.state_dict(), 'State pixel Policy-net DQN.pt')\n",
    "    torch.save(target_net.state_dict(), 'State pixel Target-net DQN.pt')\n",
    "    print('Complete')\n",
    "    return episode_durations, rewards, success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2334c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_durations, rewards, success_rates = Agent(num_episodes=12500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
